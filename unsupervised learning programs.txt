# ============================================================
# Consolidated Chapter 3 Code (Unsupervised Learning)
# - Preprocessing / Scaling
# - PCA / NMF / t-SNE
# - Clustering (k-means, Agglomerative, DBSCAN)
# - Evaluation (ARI, Silhouette)
#
# Requires: numpy, matplotlib, scikit-learn
# Optional: scipy (for dendrogram), internet for LFW faces download
# ============================================================

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer, load_digits, make_blobs, make_moons
from sklearn.model_selection import train_test_split

# ----------------------------
# 3.3 Preprocessing & Scaling
# ----------------------------
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer

def show_scaler_ranges(X_train, X_test, scaler, scaler_name="Scaler"):
    """
    Fit scaler on training set only (IMPORTANT),
    transform train and test with same scaler,
    then print per-feature min/max before and after.
    """
    scaler.fit(X_train)  # learn scaling parameters from TRAIN ONLY
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print("\n" + "="*70)
    print(f"{scaler_name}")
    print("Train shape:", X_train.shape, "Test shape:", X_test.shape)
    print("Train min before:", np.round(X_train.min(axis=0)[:5], 3), "...")
    print("Train max before:", np.round(X_train.max(axis=0)[:5], 3), "...")
    print("Train min after: ", np.round(X_train_scaled.min(axis=0)[:5], 3), "...")
    print("Train max after: ", np.round(X_train_scaled.max(axis=0)[:5], 3), "...")
    print("Test  min after: ", np.round(X_test_scaled.min(axis=0)[:5], 3), "...")
    print("Test  max after: ", np.round(X_test_scaled.max(axis=0)[:5], 3), "...")
    print("="*70)

    return X_train_scaled, X_test_scaled


# Use breast cancer dataset for scaling demos
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, random_state=1
)

# Demonstrate different scalers
_ = show_scaler_ranges(X_train, X_test, StandardScaler(), "StandardScaler (mean=0, var=1)")
_ = show_scaler_ranges(X_train, X_test, RobustScaler(), "RobustScaler (median & IQR; robust to outliers)")
_ = show_scaler_ranges(X_train, X_test, MinMaxScaler(), "MinMaxScaler (train features mapped ~[0,1])")

# Normalizer scales each ROW (each sample vector) to length 1 (direction matters)
normalizer = Normalizer()
normalizer.fit(X_train)  # technically doesn't learn much, but keeps interface consistent
X_train_norm = normalizer.transform(X_train)
print("\nNormalizer: example row norms (should be ~1.0):",
      np.round(np.linalg.norm(X_train_norm[:5], axis=1), 3))

# ---------------------------------------------------------
# 3.3.4 Effect of Preprocessing on Supervised Learning (SVC)
# ---------------------------------------------------------
from sklearn.svm import SVC

# Train SVC without scaling (often poor when features have different scales)
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    cancer.data, cancer.target, random_state=0
)

svm = SVC(C=100)
svm.fit(X_train2, y_train2)
print("\nSVC accuracy WITHOUT scaling:", round(svm.score(X_test2, y_test2), 3))

# Train SVC with MinMax scaling
mm = MinMaxScaler()
mm.fit(X_train2)
X_train2_mm = mm.transform(X_train2)
X_test2_mm = mm.transform(X_test2)

svm.fit(X_train2_mm, y_train2)
print("SVC accuracy WITH MinMax scaling:", round(svm.score(X_test2_mm, y_test2), 3))

# Train SVC with Standard scaling
ss = StandardScaler()
ss.fit(X_train2)
X_train2_ss = ss.transform(X_train2)
X_test2_ss = ss.transform(X_test2)

svm.fit(X_train2_ss, y_train2)
print("SVC accuracy WITH StandardScaler:", round(svm.score(X_test2_ss, y_test2), 3))

# ---------------------------------------------------------
# Best Practice Shortcut: Pipeline (preprocess + model)
# ---------------------------------------------------------
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), SVC(C=100))
pipe.fit(X_train2, y_train2)
print("\nPipeline(StandardScaler -> SVC) accuracy:", round(pipe.score(X_test2, y_test2), 3))


# ----------------------------
# 3.4.1 PCA (Visualization)
# ----------------------------
from sklearn.decomposition import PCA

# PCA works better when features are standardized
scaler = StandardScaler()
X_scaled = scaler.fit_transform(cancer.data)

pca2 = PCA(n_components=2, random_state=0)
X_pca = pca2.fit_transform(X_scaled)

print("\nPCA original shape:", X_scaled.shape, "reduced shape:", X_pca.shape)
print("PCA components_ shape:", pca2.components_.shape)

# Scatter plot of PCA (colored by class)
plt.figure(figsize=(7, 7))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cancer.target, s=18)
plt.xlabel("First principal component")
plt.ylabel("Second principal component")
plt.title("Breast Cancer data projected to 2D via PCA")
plt.show()

# Heatmap of PCA components (loadings)
plt.figure(figsize=(12, 3))
plt.imshow(pca2.components_, aspect='auto')
plt.yticks([0, 1], ["PC1", "PC2"])
plt.colorbar(label="Loading weight")
plt.xticks(range(len(cancer.feature_names)), cancer.feature_names, rotation=60, ha='right')
plt.title("PCA component loadings (which original features contribute to PCs)")
plt.tight_layout()
plt.show()


# ----------------------------
# 3.4.3 t-SNE (Digits demo)
# ----------------------------
from sklearn.manifold import TSNE

digits = load_digits()

# PCA to 2D (baseline visualization)
pca_digits = PCA(n_components=2, random_state=0)
digits_pca = pca_digits.fit_transform(digits.data)

plt.figure(figsize=(8, 8))
plt.scatter(digits_pca[:, 0], digits_pca[:, 1], c=digits.target, s=12)
plt.title("Digits dataset - PCA to 2D")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

# t-SNE (usually gives much clearer separation)
# NOTE: t-SNE can be slow; keep defaults for learning.
tsne = TSNE(random_state=42)
digits_tsne = tsne.fit_transform(digits.data)

plt.figure(figsize=(8, 8))
plt.scatter(digits_tsne[:, 0], digits_tsne[:, 1], c=digits.target, s=12)
plt.title("Digits dataset - t-SNE to 2D")
plt.xlabel("t-SNE feature 0")
plt.ylabel("t-SNE feature 1")
plt.show()


# ----------------------------
# 3.5 Clustering
# ----------------------------
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics.cluster import adjusted_rand_score, silhouette_score

# Use two_moons to show complex shape clustering
X_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=0)

# Scaling helps DBSCAN (and helps distance-based methods generally)
X_moons_scaled = StandardScaler().fit_transform(X_moons)

# KMeans on moons (often struggles with non-spherical clusters)
kmeans = KMeans(n_clusters=2, random_state=0)
km_labels = kmeans.fit_predict(X_moons_scaled)

# Agglomerative on moons (also struggles with complex shapes)
agg = AgglomerativeClustering(n_clusters=2)
agg_labels = agg.fit_predict(X_moons_scaled)

# DBSCAN on moons (often works well for this dataset)
db = DBSCAN()  # defaults eps=0.5, min_samples=5
db_labels = db.fit_predict(X_moons_scaled)

# Plot results
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
axes[0].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=km_labels, s=30)
axes[0].set_title("KMeans clustering")

axes[1].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=agg_labels, s=30)
axes[1].set_title("Agglomerative clustering")

axes[2].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=db_labels, s=30)
axes[2].set_title("DBSCAN clustering (noise labeled -1)")

for ax in axes:
    ax.set_xlabel("Feature 0")
    ax.set_ylabel("Feature 1")

plt.tight_layout()
plt.show()

# Evaluation with ground truth (ARI) - only available because this is synthetic
print("\nARI scores on two_moons (higher is better, 1.0 is perfect):")
print("KMeans ARI:", round(adjusted_rand_score(y_moons, km_labels), 3))
print("Agglomerative ARI:", round(adjusted_rand_score(y_moons, agg_labels), 3))
# DBSCAN includes noise label -1; ARI handles it fine
print("DBSCAN ARI:", round(adjusted_rand_score(y_moons, db_labels), 3))

# Evaluation without ground truth: silhouette (can be misleading for complex shapes)
# IMPORTANT: silhouette requires at least 2 clusters and no weird degenerate case.
def safe_silhouette(X, labels, name):
    unique = np.unique(labels)
    # silhouette cannot be computed with 1 cluster, or all noise
    if len(unique) < 2 or (len(unique) == 1 and unique[0] == -1):
        print(f"{name} silhouette: not defined (need >=2 clusters)")
        return
    print(f"{name} silhouette:", round(silhouette_score(X, labels), 3))

print("\nSilhouette scores on two_moons (can contradict intuition):")
safe_silhouette(X_moons_scaled, km_labels, "KMeans")
safe_silhouette(X_moons_scaled, agg_labels, "Agglomerative")
safe_silhouette(X_moons_scaled, db_labels, "DBSCAN")


# ---------------------------------------------------------
# Common Mistake: Accuracy is wrong for clustering labels
# ---------------------------------------------------------
from sklearn.metrics import accuracy_score

clusters1 = [0, 0, 1, 1, 0]
clusters2 = [1, 1, 0, 0, 1]

print("\nClustering label pitfall:")
print("Accuracy (WRONG metric here):", round(accuracy_score(clusters1, clusters2), 3))
print("ARI (correct idea):", round(adjusted_rand_score(clusters1, clusters2), 3))


# ============================================================
# OPTIONAL SECTION: Dendrogram (Agglomerative) via SciPy
# ============================================================
# If you have SciPy installed, you can visualize hierarchical clustering:
# from scipy.cluster.hierarchy import dendrogram, ward
#
# X_small, _ = make_blobs(random_state=0, n_samples=12)
# linkage_array = ward(X_small)
# plt.figure(figsize=(10, 4))
# dendrogram(linkage_array)
# plt.xlabel("Sample index")
# plt.ylabel("Cluster distance")
# plt.title("Dendrogram (Ward linkage)")
# plt.show()


# ============================================================
# OPTIONAL SECTION: Faces (LFW) - PCA(whiten) + KNN + NMF
# This downloads data and may be slow / require internet.
# ============================================================
# from sklearn.datasets import fetch_lfw_people
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.decomposition import NMF
#
# people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
# image_shape = people.images[0].shape
#
# # Limit to <=50 images per person to reduce skew
# mask = np.zeros(people.target.shape, dtype=bool)
# for target in np.unique(people.target):
#     mask[np.where(people.target == target)[0][:50]] = True
#
# X_people = people.data[mask] / 255.0
# y_people = people.target[mask]
#
# X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(
#     X_people, y_people, stratify=y_people, random_state=0
# )
#
# # Baseline 1-NN on pixels (often poor)
# knn = KNeighborsClassifier(n_neighbors=1)
# knn.fit(X_train_f, y_train_f)
# print("\nFaces: 1-NN on raw pixels accuracy:", round(knn.score(X_test_f, y_test_f), 3))
#
# # PCA whitening + 1-NN (often improves)
# pca = PCA(n_components=100, whiten=True, random_state=0)
# pca.fit(X_train_f)
# X_train_pca = pca.transform(X_train_f)
# X_test_pca = pca.transform(X_test_f)
#
# knn.fit(X_train_pca, y_train_f)
# print("Faces: 1-NN on PCA-whitened features accuracy:", round(knn.score(X_test_pca, y_test_f), 3))
#
# # NMF components (requires nonnegative data; faces are nonnegative already)
# nmf = NMF(n_components=15, random_state=0, max_iter=1000)
# nmf.fit(X_train_f)
# components = nmf.components_
#
# # Visualize first 15 NMF components
# fig, axes = plt.subplots(3, 5, figsize=(12, 8), subplot_kw={'xticks': (), 'yticks': ()})
# for i, ax in enumerate(axes.ravel()):
#     ax.imshow(components[i].reshape(image_shape), cmap='gray')
#     ax.set_title(f"Component {i}")
# plt.suptitle("NMF components (faces)")
# plt.tight_layout()
# plt.show()
#
# # Show images with high activation for a chosen component
# X_train_nmf = nmf.transform(X_train_f)
# compn = 3
# inds = np.argsort(X_train_nmf[:, compn])[::-1][:10]
# fig, axes = plt.subplots(2, 5, figsize=(12, 5), subplot_kw={'xticks': (), 'yticks': ()})
# for idx, ax in zip(inds, axes.ravel()):
#     ax.imshow(X_train_f[idx].reshape(image_shape), cmap='gray')
# plt.suptitle(f"Top images for NMF component {compn}")
# plt.tight_layout()
# plt.show()

print("\nDone. This script covers the main code patterns for Chapter 3.")