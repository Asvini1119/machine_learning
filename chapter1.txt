
"""
Chapter 1 (Introduction) — Consolidated Code (with comments)
Source: "Introduction to Machine Learning with Python" — Chapter 1 examples

How to use:
- Best run in a Jupyter Notebook.
- If running as a .py script, remove the notebook magic lines and add plt.show() where noted.
"""

# ---------------------------------------------------------------------
# 0) Common imports used throughout the book (as the chapter notes)
# ---------------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import display  # pretty printing in notebooks

# Optional (used for a colormap in the scatter_matrix example)
# If you don't have mglearn installed, comment these lines and remove cmap=...
try:
    import mglearn
    HAS_MGLEARN = True
except Exception:
    HAS_MGLEARN = False


# ---------------------------------------------------------------------
# 1) NumPy example: creating and printing a NumPy array
# ---------------------------------------------------------------------
x = np.array([[1, 2, 3], [4, 5, 6]])
print("NumPy array x:\n{}".format(x))


# ---------------------------------------------------------------------
# 2) SciPy example: sparse matrices (CSR and COO formats)
# ---------------------------------------------------------------------
from scipy import sparse

# Create a 2D NumPy array with a diagonal of ones and zeros elsewhere
eye = np.eye(4)
print("\nDense NumPy array (eye):\n", eye)

# Convert the dense array to a SciPy sparse matrix (CSR format)
sparse_matrix = sparse.csr_matrix(eye)
print("\nSciPy sparse CSR matrix:\n", sparse_matrix)

# Create the same sparse matrix directly using COO format (recommended for large sparse data)
data = np.ones(4)
row_indices = np.arange(4)
col_indices = np.arange(4)
eye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))
print("\nCOO representation:\n", eye_coo)


# ---------------------------------------------------------------------
# 3) matplotlib example: simple sine plot
# ---------------------------------------------------------------------
# In Jupyter you can enable inline plots:
# %matplotlib inline
# or interactive plots:
# %matplotlib notebook

# Generate x from -10 to 10 (100 points)
x = np.linspace(-10, 10, 100)
# Compute sine wave
y = np.sin(x)

# Plot y vs x
plt.figure()
plt.plot(x, y, marker="x")
plt.title("Sine plot (Chapter 1 example)")
plt.xlabel("x")
plt.ylabel("sin(x)")
# If running as a script, uncomment:
# plt.show()


# ---------------------------------------------------------------------
# 4) pandas example: DataFrame creation + filtering rows
# ---------------------------------------------------------------------
data = {
    "Name": ["John", "Anna", "Peter", "Linda"],
    "Location": ["New York", "Paris", "Berlin", "London"],
    "Age": [24, 13, 53, 33],
}
data_pandas = pd.DataFrame(data)

print("\nFull DataFrame:")
display(data_pandas)

print("\nRows where Age > 30:")
display(data_pandas[data_pandas.Age > 30])


# ---------------------------------------------------------------------
# 5) Checking library versions (as shown in the chapter)
# ---------------------------------------------------------------------
import sys
import scipy as sp
import IPython
import sklearn

print("\n--- Versions (informational) ---")
print("Python version:", sys.version)
print("pandas version:", pd.__version__)
print("matplotlib version:", plt.matplotlib.__version__)
print("NumPy version:", np.__version__)
print("SciPy version:", sp.__version__)
print("IPython version:", IPython.__version__)
print("scikit-learn version:", sklearn.__version__)


# ---------------------------------------------------------------------
# 6) First ML application: Iris classification with k-NN (end-to-end)
# ---------------------------------------------------------------------
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# Load iris dataset (returns a "Bunch" similar to a dict)
iris_dataset = load_iris()

print("\n--- Iris dataset keys ---")
print("Keys of iris_dataset:\n", iris_dataset.keys())

# Dataset description (first part)
print("\n--- Iris dataset description (first 193 chars) ---")
print(iris_dataset["DESCR"][:193] + "\n...")

print("\nTarget names:", iris_dataset["target_names"])
print("Feature names:\n", iris_dataset["feature_names"])

# Data and target shapes
print("\nType of data:", type(iris_dataset["data"]))
print("Shape of data:", iris_dataset["data"].shape)
print("First five rows of data:\n", iris_dataset["data"][:5])

print("\nType of target:", type(iris_dataset["target"]))
print("Shape of target:", iris_dataset["target"].shape)
print("Target (encoded labels):\n", iris_dataset["target"])

# Split into train/test sets (default: 75% train, 25% test)
X_train, X_test, y_train, y_test = train_test_split(
    iris_dataset["data"], iris_dataset["target"], random_state=0
)

print("\nTrain/Test shapes:")
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Visual inspection: pair plot (scatter matrix)
# Convert training features to a DataFrame with correct column names
iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)

# In the book: pd.plotting.scatter_matrix(..., cmap=mglearn.cm3)
# If mglearn is unavailable, we still plot without that colormap.
plt.figure()
if HAS_MGLEARN:
    pd.plotting.scatter_matrix(
        iris_dataframe,
        c=y_train,
        figsize=(10, 10),
        marker="o",
        hist_kwds={"bins": 20},
        s=60,
        alpha=0.8,
        cmap=mglearn.cm3,
    )
else:
    pd.plotting.scatter_matrix(
        iris_dataframe,
        c=y_train,
        figsize=(10, 10),
        marker="o",
        hist_kwds={"bins": 20},
        s=60,
        alpha=0.8,
    )
plt.suptitle("Iris scatter matrix (colored by class)", y=1.02)
# If running as a script, uncomment:
# plt.show()

# Build the k-NN classifier (k=1, as in Chapter 1)
knn = KNeighborsClassifier(n_neighbors=1)

# Train the model (fit)
knn.fit(X_train, y_train)

# Predict a NEW sample (one iris measurement row: 1 sample x 4 features)
X_new = np.array([[5, 2.9, 1, 0.2]])
print("\nX_new.shape:", X_new.shape)

prediction = knn.predict(X_new)
print("Prediction (class id):", prediction)
print("Predicted target name:", iris_dataset["target_names"][prediction])

# Evaluate: predict on test set and compute accuracy
y_pred = knn.predict(X_test)
print("\nTest set predictions:\n", y_pred)

# Accuracy via manual mean (as in the chapter)
print("Test set score (manual): {:.2f}".format(np.mean(y_pred == y_test)))

# Accuracy via the model's score method (recommended)
print("Test set score (knn.score): {:.2f}".format(knn.score(X_test, y_test)))


# ---------------------------------------------------------------------
# 7) Chapter 1 "core workflow" summary snippet (exact structure shown)
# ---------------------------------------------------------------------
# NOTE: The chapter shows this as the minimal standard recipe:
X_train, X_test, y_train, y_test = train_test_split(
    iris_dataset["data"], iris_dataset["target"], random_state=0
)

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

print("\nCore workflow test score: {:.2f}".format(knn.score(X_test, y_test)))

print("Test set score:", knn.score(X_test, y_test))



import numpy as np

X_new = np.array([[5, 2.9, 1, 0.2]])

prediction = knn.predict(X_new)

print("Predicted target name:",
      iris_dataset['target_names'][prediction])

